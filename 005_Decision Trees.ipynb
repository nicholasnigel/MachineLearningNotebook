{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "## Outline\n",
    "1. [Training & Visualizing Decision Tree](#part1)\n",
    "2. [Estimating Class Probabilities](#part2)\n",
    "3. [Cart Training Algorithm](#part3)\n",
    "4. [Analysis](#part4)\n",
    "5. [Gini Impurity or Entropy](#part5)\n",
    "6. [Regularization Hyperparameters](#part6)\n",
    "7. [Regression](#part7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='part1'>Part 1</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:,2:] # the third column\n",
    "y = iris.target\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing purposes \n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(\n",
    "    tree_clf,\n",
    "    out_file='iris_tree.dot',\n",
    "    feature_names = iris.feature_names[2:],\n",
    "    class_names = iris.target_names,\n",
    "    rounded=True,\n",
    "    filled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After obtaining .dot file, we can convert them into pdf or png.  \n",
    "```\n",
    "dot -Tpng iris_tree.dot -o iris_tree.png\n",
    "```\n",
    "Result would look like:  \n",
    "\n",
    "<img src='iris_tree.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__How to read the Figure above__:\n",
    "\n",
    "Given an instance (with its respective features) we start from the root node. Compare the flower's petal length. If it is smaller than 2.45cm, follow the True option. If it leaf node already, then look at the class represented (setosa).\n",
    "\n",
    "If length is greater than 2.45 instead, then follow the False route. If not leaf node, keep asking again to compare with the condition, and do the same thing as before. \n",
    "\n",
    "One advantage of doing decision tree is that features don't need to be scaled. And that they require little to no data preparation.\n",
    "\n",
    "__Attributes of each node__:  \n",
    "\n",
    "_samples_: how many training instances it applies to. For example, there are 100 instances with petal length greater than 2.45 cm, and among those 54 have petal width smaller than 1.75 cm.\n",
    "\n",
    "_value_: Number of training instances of each class this node applies to. Say for the node in bottom right, there are 0 Iris-Setosa, 1 Iris-Versicolor, and 45 Iris-Virginica.  \n",
    "\n",
    "_gini_: measure impurity of a node, where impurity means that gini index is 0 (where all training instances it applies to belong to the same class). \n",
    "\n",
    "Calculating impurity:  \n",
    "$$\n",
    "G_i = 1 - \\sum_{k=1}^{n}p_{i,k}^2\n",
    "$$  \n",
    "where $p_{i,k}$ is the ratio of class k instances among the training isntance of the i<sup>th</sup> node.  \n",
    "    \n",
    "0 Impurity means that like left node of depth 1, all 50 of the instances are considered as Iris-Setosa.  \n",
    "\n",
    "Scikit learn sues the CART algorithm where it produces binary trees. But there may be other algorithms like ID3 that can produce ternary trees.  \n",
    "\n",
    "<img src='img/005_1.jpg'>  \n",
    "\n",
    "This is the graph that shows the decision boundaries for the trained Decision Tree. Thick vertical line represents decision boundary for root node. Thinner lines, means that it is impure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='part2'>Estimating Class Probabilities</a>  \n",
    "\n",
    "It can also estimate the probability that an instance belongs to a particular class k. For example, a flower with petal length 5 cm and petal width 1.5 cm. It should belong to the depth-2 left node. We look in the value attribute to see how many instances are classified into the 3 classes. 0/54 Iris-Setosa (0%), 49/54 Iris-Versicolor(90.7%), 5/54 Iris-Virginica (9.3%). When told to classify therefore, then this should belong to one with highest probability which we know is the Iris-Versicolor.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.90740741, 0.09259259]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict_proba([[5,1.5]]) # result should match the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict([[5,1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with the above code it shows that our conclusion is proven to be true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='part3'>Cart Training Algorithm</a>\n",
    "\n",
    "CART stands for _Classification and Regression Tree_. Algorihtm first splits the training set into 2 subsets using single feature k and a threshold $t_k$. It chooses the $k$ and $t_k$ by looking at the pair that produces the purest subsets.  \n",
    "\n",
    "$$\n",
    "J(k,t_k) = \\frac{m_{left}}{m}G_{left} + \\frac{m_{right}}{m}G_{right}\n",
    "$$  \n",
    "where:  \n",
    "$G_{left/right}$ measures impurity of left/right subset  \n",
    "$m_{left/right}$ is the number of instance in the left/right subset.  \n",
    "Function J is the cost function for classification.  \n",
    "\n",
    "After successfully split the training set in two, t recursively split the subsets again with same logic. It will stop until it reaches the max depth (which is determined by max_depth parameter), or if it cannot find a split that will reduce impurity.  \n",
    "\n",
    "Finding optimal tree is an _NP-Complete_ problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='part4'>Analysis</a>\n",
    "\n",
    "Assuming that training has been done and the decision tree is formed, we traverse decision tree from root to the leaf. For CART algorithm, it requires going through $O(\\log_2{m})$ nodes. For each traversal we only require the comparison between values, it takes roughly O(1) each. In summary therefore, the prediction complexity takes $O(\\log_2{m})$.  \n",
    "\n",
    "As for the training complexity, because the algorithm requires to compare all features on all samples at each node, the complexity woudl be $O(n\\times m \\log{m})$. n is number of features and _m_ is the number of training instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='part5'>Gini Impurity or Entropy</a>\n",
    "\n",
    "Previously, we use the gini index as a measure of impurity. However, we can also use entropy as measure. We can control this using the _DecisionTreeClassifier_'s hyperparameter: _criterion_. When all messages are identical, the entropy is zero. \n",
    "\n",
    "The measure is:  \n",
    "\n",
    "$$\n",
    "H_i = - \\sum_{k=1}^{n} p_{i,k} \\log{(p_{i,k})} \n",
    "$$\n",
    "where $p_{i,k} \\neq 0 $.  \n",
    "\n",
    "So for example from the decision tree figure above, the entropy for depth-2 left node is $-\\frac{49}{54}log{\\frac{49}{54}} - \\frac{5}{54}\\log{\\frac{5}{54}}$.  \n",
    "\n",
    "Gini impurity is slightly faster to compute but tends to isolate the most frequent class in its own branch of the tree, while entropy tends to produce slightly more balanced trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='part6'>Regularization Hyperparameters</a>  \n",
    "\n",
    "Decision Trees make very little assumptions about training data. If left unconstrained, tree structure will adapt itself to the training data, and likely to cause overfitting. The model is then called nonparametric model because number of parameters is not determined prior to training.  \n",
    "\n",
    "So to avoid overfitting, we restrict the freedom of Decision Tree during training by regularization. The regularization hyperparameters depend on the algorithm used.  \n",
    "\n",
    "In general, we can restrict the maximum depth (hyperparameter `max_depth`). Reduce `max_depth` will regularize the model and thus reduce the risk of overfitting.  \n",
    "\n",
    "Other parameters also include: `min_samples_splot`, `min_samples_leaf`, `min_weight_fraction_leaf`, `max_leaf_nodes`, `max_features`. Increasing the `min_` hyperparameters or reduc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='part7'>Regression</a>  \n",
    "\n",
    "We can do regression using decision tree by [Decision Tree Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_graphviz(\n",
    "    tree_reg,\n",
    "    out_file='regressor_tree.dot',\n",
    "    feature_names = iris.feature_names[2:],\n",
    "    class_names = iris.target_names,\n",
    "    rounded=True,\n",
    "    filled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
